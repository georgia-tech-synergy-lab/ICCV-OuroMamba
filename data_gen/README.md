# OuroMamba-Gen
**OuroMamba-Gen** extends the internal dynamics of the **Mamba** architecture to enable *data-free calibration sample generation* for quantization.  
This module modifies Mambaâ€™s state update mechanism to expose the **enhanced hidden state** `X_enhanced`, which aggregates local spatial context weighted by the per-token gate Î”(t), improving the implicit attention of Vision Mamba models.

---

## ğŸ” Overview

Traditional Mamba implementations compress each tokenâ€™s hidden state along the scanning direction, introducing a **local bias** that limits global token interaction.  
**OuroMamba-Gen** mitigates this by introducing *direction-agnostic spatial refinement*, effectively creating a more context-aware state representation.

### âœ¨ Key Features
- **Internal Mamba modification** â€” extends `selective_scan_ref` to compute and return the enhanced hidden state.  
- **Spatial State Fusion** â€” performs localized 3Ã—3 neighborhood aggregation on token states, weighted by input gate Î”(t).  
- **Enhanced Implicit Attention** â€” yields refined hidden states (`X_enhanced`) that produce sharper, semantically aligned attention maps.

---

## ğŸ§© Integration

After modification, the internal scan function now exposes the enhanced hidden state:

```python
# Example usage
out, X_enhanced = selective_scan_ref(u, delta, A, B, C, return_last_state=False)